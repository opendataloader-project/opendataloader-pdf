name: Test & Benchmark

on:
  pull_request:
    branches: [main]
    paths:
      - 'java/**'
      - 'tests/benchmark/**'
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '21'

      - name: Setup uv
        uses: astral-sh/setup-uv@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Setup pnpm
        run: npm install -g pnpm

      - name: Build & Test All
        run: ./scripts/build-all.sh

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          files: java/opendataloader-pdf-core/target/site/jacoco/jacoco.xml
          fail_ci_if_error: true
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: java-build
          path: java/opendataloader-pdf-cli/target/*.jar
          retention-days: 1

  benchmark:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '21'

      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: java-build
          path: java/opendataloader-pdf-cli/target/

      - name: Setup uv
        uses: astral-sh/setup-uv@v4

      - name: Fetch baseline from main
        run: |
          git fetch origin main
          git show origin/main:tests/benchmark/prediction/opendataloader/evaluation.json > /tmp/baseline.json || echo '{}' > /tmp/baseline.json

      - name: Run benchmark with regression check
        run: |
          cd tests/benchmark
          uv sync --quiet
          uv run python run.py --check-regression

      - name: Comment benchmark results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let current;
            try {
              current = JSON.parse(fs.readFileSync('tests/benchmark/prediction/opendataloader/evaluation.json', 'utf8'));
            } catch (e) {
              core.warning('Failed to read evaluation results');
              return;
            }

            let baseline = {};
            try {
              baseline = JSON.parse(fs.readFileSync('/tmp/baseline.json', 'utf8'));
            } catch (e) {}

            const m = current.metrics?.score || {};
            const b = baseline.metrics?.score || {};
            const td = current.table_detection || {};
            const btd = baseline.table_detection || {};

            const fmt = (v) => v != null ? Number(v).toFixed(3) : '-';
            const diff = (curr, base) => {
              if (curr == null || base == null) return '-';
              const pct = ((Number(curr) - Number(base)) / Number(base) * 100);
              const sign = pct >= 0 ? '+' : '';
              return `${sign}${pct.toFixed(1)}%`;
            };

            const docCount = Number(current.summary?.document_count) || 0;
            const commitSha = String(context.sha || '').substring(0, 7);

            const lines = [
              '## ðŸ“Š Benchmark Results',
              '',
              '### Scores',
              '| Metric | Score | Baseline | Change |',
              '|--------|-------|----------|--------|',
              `| **NID** | ${fmt(m.nid_mean)} | ${fmt(b.nid_mean)} | ${diff(m.nid_mean, b.nid_mean)} |`,
              `| **TEDS** | ${fmt(m.teds_mean)} | ${fmt(b.teds_mean)} | ${diff(m.teds_mean, b.teds_mean)} |`,
              `| **MHS** | ${fmt(m.mhs_mean)} | ${fmt(b.mhs_mean)} | ${diff(m.mhs_mean, b.mhs_mean)} |`,
              '',
              '### Table Detection',
              '| Metric | Score | Baseline | Change |',
              '|--------|-------|----------|--------|',
              `| **F1** | ${fmt(td.f1)} | ${fmt(btd.f1)} | ${diff(td.f1, btd.f1)} |`,
              `| **Precision** | ${fmt(td.precision)} | ${fmt(btd.precision)} | ${diff(td.precision, btd.precision)} |`,
              `| **Recall** | ${fmt(td.recall)} | ${fmt(btd.recall)} | ${diff(td.recall, btd.recall)} |`,
              '',
              '<details>',
              '<summary>Details</summary>',
              '',
              `- Documents: ${docCount}`,
              `- Commit: \`${commitSha}\``,
              '',
              '</details>',
            ];
            const body = lines.join('\n');

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });

      - name: Upload evaluation results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: tests/benchmark/prediction/opendataloader/evaluation.json
