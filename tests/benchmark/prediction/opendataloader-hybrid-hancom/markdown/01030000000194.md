Peihao Wang, Rameswar Panda, Lucas Torroba Hen-
nigen, Philip Greengard, Leonid Karlinsky, Roge-
rio Feris, David Daniel Cox, Zhangyang Wang, and
Yoon Kim. 2023. Learning to grow pretrained mod-
els for efficient transformer training. arXiv preprint
arXiv:2303.00980,

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, A1-
isa Liu, Noah ASmith, Daniel Khashabi, and Han-
naneh Hajishirzi. 2022. Self-instruct: Aligning lan-
guage model with self generated instructions. arXiv
preprint arXiv:2212.10560.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al.
2022a. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
etal. 2022b. Chain-of-thought prompting elicits rea-
soningin large language models. Advances inNeural
Information Processing Systems, 35:24824-24837.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi,Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,
et al. 2019. Huggingface transformers: State-of-
the-art natural language processing. arXivpreprint
arXiv:1910.03771.

Prateek Yadav, Derek Tam, Leshem Choshen, Colin
Raffel,and Mohit Bansal 2023. Ties-merging: Re-
solving interference when merging models. In Thirty-
seventh Conference on Neural Information Process-
ing Systems.

Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,
Quoc V Le, Denny Zhou, and Xinyun Chen. 2023.
Large language models as optimizers. arXiv preprint
arXiv:2309.03409.

Yiqun Yao, Zheng Zhang, Jing Li, and Yequan
Wang. 2023. 2x faster language model pre-training
via masked structural growth. arXiv preprint
arXiv:2305.02869.

Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,
Zhengying Liu, Yu Zhang, James T Kwok, Zhen-
guo Li, Adrian Weller, and Weiyang Liu. 2023.
Metamath: Bootstrap your own mathematical ques-
tions for large language models. arXiv preprint
arXiv:2309.12284.

Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,
Songfang Huang, and Fei Huang. 2023. Rrhf:
Rank responses to align language models with
human feedback without tears. arXiv preprint
arXiv:2304.05302.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Cana
machine really finish your sentence? InProceedings
of the 57th Annual Meeting of the Associationfor
Computational Linguistics, pages 4791-4800.

Shengyu Zhang, Linfeng Dong, XiaoyaLi, Sen Zhang,
Xiaofei Sun, Shuhe Wang, JiweiLi, Runyi Hu, Tian-
wei Zhang, Fei Wu, et al. 2023. Instruction tuning
forlarge language models: A survey. arXivprrprint
arXiv:2308.10792.

Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min,Beichen
Zhang, Junjie Zhang, Zican Dong, et al. 2023. A
survey of large language models. arXiv preprint
arXiv:2303.18223.

Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen,
Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong
Wen, and Jiawei Han. 2023. Don'tmake your 1lm
an evaluation benchmark cheater. arXiv preprint
arXiv:2311.01964.

Daniel M Ziegler, Nisan Stiennon,Jeffrey Wu,TomB
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
guage models from human preferences. arXiv
preprint arXiv:1909.08593.

