|Properties|Instruction| | |Training Datasets| | |
|---|---|---|---|---|---|---|
| |Alpaca-GPT4|OpenOrca|Synth. Math-Instruct|OrcaDPO Pairs|Ultrafeedback Cleaned|Synth. Math-Alignment|
| |52K|2.91M|126K|12.9K|60.8K|126K|
| |52K|100K|52K|12.9K|60.8K|20.1K|
| |0|0| |0|0| |


Table 1: Training datasets used for the instruction and alignment tuning stages, respectively. For the instruction
tuning process, we utilized the Alpaca-GPT4 (Peng et al., 2023), OpenOrca (Mukherjee et al., 2023), and Synth.
Math-Instruct datasets, while forthe alignment tuning, we employed the Orca DPO Pairs (Intel,2023), Ultrafeedback
Cleaned (Cui etal., 2023; Ivison et al.,2023), and Synth. Math-Alignment datasets. The "Total #Samples' indicates
the total number of samples in the entire dataset. The "Maximum #Samples Used' indicates the actual maximum
number of samples that were used in training, which could be lower than the total numberof samples in a given
dataset. OOpen Source* indicates whether the dataset is open-sourced

pretraining to quickly recover performance. We
attribute the success of DUS to reducing such dis-
crepancies in both the depthwise scaling and the
continued pretraining steps. We also hypothesize
that other methods of depthwise scaling could also
work for DUS, as long as the discrepancy in the
scaled model is sufficiently contained before the
continued pretraining step.

Comparison to other up-scaling methods. Un-
like Komatsuzaki et al. (2022), depthwise scaled
models do not require additional modules like gat-
ing networks or dynamic expert selection. Conse-
quently, scaled models in DUS do not necessitate
a distinct training framework for optimal training
efficiency, nor do they require specialized CUDA
kernels for fast inference. A DUS model can seam-
lessly integrate into existing training and inference
frameworks while maintaining high efficiency.

# 3 Training Details

After DUS, including continued pretraining, we
perform fine-tuning of SOLAR 10.7B in two stages:
1)instruction tuning and 2) alignment tuning.

Instruction tuning. In the instruction tuning
stage, the model is trained to follow instructions 1n
a QA format (Zhang et al., 2023b). We mostly use
open-source datasets but also synthesize math QA
dataset to enhance the model's mathematical capa-
bilities. A rundown ofhow we crafted the dataset 1S
as follows. First, seed math data are collected from
the Math (Hendrycks et al., 2021) dataset only, to
avoid contamination with commonly used bench-
mark datasets such as GSM8K (Cobbe etal.,2021).
Then, using a process similar to MetaMath (Yu
et al., 2023), we rephrase the questions and an-
swers of the seed math data. We use the resulting
rephrased question-answer pairs as a QA dataset

and call it *Synth. Math-Instruct*

Alignment tuning. In the alignment tuning stage,
the instruction-tuned model is further fine-tuned to
be more aligned with human or strong AI (e.g.,
GPT4 (OpenAI, 2023)) preferences using direct
preference optimization (DPO) (Rafailov et al.,
2023). Similar to the instruction tuning stage, we
use mostly open-source datasets but also synthe-
size a math-focused alignment dataset utilizing the
*Synth. Math-Instruct" dataset mentioned in the
instruction tuning stage.

The alignment data synthesis process is as
follows. We take advantage of the fact that
the rephrased question-answer pairs in Synth.
Math-Instruct data are beneficial in enhancing the
model's mathematical capabilities (see Sec. 4.3.1).
Thus, we speculate that the rephrased answer to the
rephrased question is a better answer than the orig-
inal answer, possibly due to the interim rephrasing
step. Consequently, we set the rephrased question
as the prompt and use the rephrased answer as the
chosen response and the original answer as the re-
jected response and create the [prompt, chosen,
rejected] DPO tuple. We aggregate the tuples from
the rephrased question-answer pairs and call the
resulting dataset 'Synth. Math-Alignment"

# 4 Results

# 4.1 Experimental Details

Training datasets. We present details regarding
our training datasets for the instruction and align-
ment tuning stages in Tab. 1. We do not always
use the entire dataset and instead subsample a set
amount. Note that most of our training data is
open-source, and the undisclosed datasets can be
substituted for open-source alternatives such as the
MetaMathQA (Yuetal.,2023) dataset.

