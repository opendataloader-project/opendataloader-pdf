Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawa-
har, Sahaj Agarwal, Hamid Palangi, and Ahmed
Awadallah. 2023. Orca: Progressive learning from
complex explanation traces of gpt-4. arXiv preprint
arXiv:2306.02707.

# OpenAI. 2023. Gpt-4 technical report.

Yu Pan, Ye Yuan, Yichun Yin, Zenglin Xu, Lifeng
Shang, XinJiang, and Qun Liu. 2023. Reusing pre-
trained models by multi-linear operators forefficient
training. arXiv preprint arXiv:2310.10699.

Baolin Peng, Chunyuan Li, Pengcheng He,Michel Gal-
ley, and Jianfeng Gao. 2023. Instruction tuning with
gpt-4. arXiv preprint arXiv:2304.03277.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.

Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, et al. 2021. Scaling language models:
Methods, analysis & insights from training gopher.
arXiv preprint arXiv:2112.11446.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
Ermon, Christopher D Manning, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model. arXiv preprint
arXiv:2305.18290,

Oscar Sainz, Jon Ander Campos, Iker Garcia-Ferrero,
Julen Etxaniz, Oier Lopez de Lacalle, and Eneko
Agirre. 2023. Nlp evaluation in trouble: On the
need to measure 1lm data contamination for each
benchmark. arXiv preprint arXiv:2310.18018.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-
ula, and Yejin Choi. 2021. Winogrande: An adver-
sarial winograd schema challenge at scale. Commu-
nications ofthe ACM, 64(9):99-106.

Malik Sallam, Nesreen Salim, Muna Barakat, and Alaa
Al-Tammemi. 2023. Chatgpt applications in medical,
dental, pharmacy, and public health education: A
descriptive study highlighting the advantages and
limitations. NarraJ, 3(1):e103--103.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
Dean. 2017. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. arXiv
preprintarXiv:1701.06538.

Tianxiao Shen, Myle Ott, Michael Auli, and
Marc"Aurelio Ranzato. 2019. Mixture models for
diverse machine translation: Tricks ofthe trade. In
International conference on machine learning, pages
5719-5728.PMLR.

Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo
Huang, Daogao Liu, Terra Blevins, Danqi Chen,
and Luke Zettlemoyer. 2023. Detecting pretraining
data from large language models. arXiv preprint
arXiv:2310.16789.

Ken Shoemake. 1985. Animating rotation with quater-
nion curves. In Proceedings ofthe 12th annual con-
ference on Computer graphics and interactive tech-
niques, pages 245-254.

Mingxing Tan and Quoc Le. 2019. Efficientnet: Re-
thinking model scaling for convolutional neural net-
works. In International conference on machine learn-
ing, pages 6105-6114.PMLR.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra,Prajjwal Bhargava,Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.

Lewis Tunstall, Edward Beeching, Nathan Lambert,
Nazneen Rajani, Kashif Rasul, Younes Belkada,
Shengyi Huang, Leandro von Werra, Clementine
Fourrier, Nathan Habib, et al. 2023. Zephyr: Di-
rect distillation of lm alignment. arXiv preprint
arXiv:2310.16944.

Peihao Wang, Rameswar Panda, Lucas Torroba Hen-
nigen, Philip Greengard, Leonid Karlinsky, Roge-
rio Feris, David Daniel Cox, Zhangyang Wang, and
Yoon Kim. 2023. Learning to grow pretrained mod-
els for efficient transformer training. arXivpreprint
arXiv:2303.00980,

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra,Al-
isa Liu, Noah A Smith, Daniel Khashabi, and Han-
naneh Hajishirzi. 2022. Self-instruct: Aligning lan-
guage model with self generated instructions. arXiv
preprint arXiv:2212.10560.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXivpreprint
arXiv:2109.01652.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, etal.
2022a. Emergent abilities oflarge language models.
arXiv preprint arXiv:2206.07682.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022b. Chain-of-thought prompting elicits rea-
soning in large language models. Advances inNeural
Information Processing Systems, 35:24824-24837.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi,Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,
et al. 2019. Huggingface's transformers: State-of-
the-art natural language processing. arXivpreprint
arXiv:1910.03771.

