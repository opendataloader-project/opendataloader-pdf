

Figure 1: Depth up-scaling for the case with n 32.s 48, and M 8. Depth up-scaling is achieved through a
dual-stage process of depthwise scaling followed by continued pretraining.

for wider access and application of these models
by researchers and developers globally.

# 2 Depth Up-Scaling

Toefficiently scale-up LLMs, we aim to utilize pre-
trained weights of base models to scale up to larger
LLMs (Komatsuzaki et al., 2022). While exist-
ing methods such as Komatsuzaki et al. (2022) use
MoE (Shazeer etal., 2017) to scale-up the model ar-
chitecture, we opt fora different depthwise scaling
strategy inspired by Tan and Le (2019). We then
continually pretrain the scaled model as just scaling
the model without further pretraining degrades the
performance.

Base model. Any n-layer transformer architec-
ture can be used but we select the 32-layer Llama
2 architecture as our base model. We initialize the
Llama 2 architecture with pretrained weights from
Mistral 7B, as itis one of the top performers com-
patible with the Llama 2 architecture. By adopting
the Llama 2 architecture for our base model, we
aim to leverage the vast pool of community re-
sources while introducing novel modifications to
further enhance its capabilities.

Depthwise scaling. From the base model with N
layers, we set the target layer count S for the scaled
model, which is largely dictated by the available
hardware.

With the above, the depthwise scaling process
is as follows. The base model with N layers is
duplicated for subsequent modification. Then, we
remove the final M layers from the original model
and the initial M layers from its duplicate, thus
forming two distinct models with n M layers.
These two models are concatenated to forma scaled
model withss 2Â·(n-m) layers. Note thatn 32
from ourbase model and we sets = 48 considering

our hardware constraints and the efficiency of the
scaled model, i.e., fitting between 7 and 13 billion
parameters. Naturally, this leads to the removal of
M 8 layers. The depthwise scaling process with
n 32, 48, and M 8is depicted in 'Step 1:
Depthwise Scaling ofFig. 1.

We note thata methodin the community thatalso
scale the model in the same manner as 'Step 1:
Depthwise Scaling" ofFig. 1 has been concurrently
developed.

Continued pretraining. The performance of the
depthwise scaled model initially drops below that
of the base LLM. Thus, we additionally apply
the continued pretraining step as shown in 'Step
2: Continued Pretraining" ofFig. 1. Experimen-
tally, we observe rapid performance recovery of
the scaled model during continued pretraining, a
phenomenon also observed in Komatsuzaki et al.
(2022). We consider that the particular way of
depthwise scaling has isolated the heterogeneity
in the scaled model which allowed for this fast
performance recovery.

Delving deeper into the heterogeneity of the
scaled model, a simpler alternative to depthwise
scaling could be to just repeat its layers once more,
i.e., from N to 2n layers. Then, the 'layer distance'
or the difference in the layer indices in the base
model, is only bigger than 1 where layers N and
n + 1 are connected, i.e., at the seam.

However, this results in maximum layer distance
at the seam, which may be too significant of a
discrepancy for continued pretraining to quickly
resolve. Instead, depthwise scaling sacrifices the
2m middle layers, thereby reducing the discrep-
ancy at the seam and makingit easier for continued

Mistra1-11B-v0.1

