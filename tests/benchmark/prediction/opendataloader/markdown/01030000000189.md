Model Alpaca-GPT4 OpenOrca Synth. Math-Instruct H6 (Avg.) ARC HellaSwag MMLU TruthfulQA Winogrande GSM8K

SFT v1 O ✗ ✗ 69.15 67.66 86.03 65.88 60.12 82.95 52.24 SFT v2 O O ✗ 69.21 65.36 85.39 65.93 58.47 82.79 57.32 SFT v3 O O O 70.03 65.87 85.55 65.31 57.93 81.37 64.14 SFT v4 O ✗ O 70.88 67.32 85.87 65.87 58.97 82.48 64.75 SFT v3 + v4 O O O 71.11 67.32 85.96 65.95 58.80 2.08 66.57

- Table 3: Ablation studies on the different datasets used for instruction tuning. ‘SFT v3+v4’ indicates that the model is merged from ‘SFT v3’ and ‘SFT v4’ by simply averaging the model weights. The best scores for H6 and the individual tasks are shown in bold.

Model Ultrafeedback Clean Synth. Math-Alignment H6 (Avg.) ARC HellaSwag MMLU TruthfulQA Winogrande GSM8K

DPO v1 O ✗ 73.06 71.42 88.49 66.14 72.04 81.45 58.83 DPO v2 O O 73.42 71.50 88.28 65.97 71.71 82.79 60.27 DPO v1 + v2 O O 73.21 71.33 88.36 65.92 72.65 82.79 58.23

- Table 4: Ablation studies on the different datasets used during the direct preference optimization (DPO) stage. ‘SFT v3’ is used as the SFT base model for DPO. We name ablated models with the ‘DPO’ prefix to indicate the alignment tuning stage. ‘DPO v1+v2’ indicates that the model is merged from ‘DPO v1’ and ‘DPO v2’ by simply averaging the model weights. The best scores for H6 and the individual tasks are shown in bold.

Model Base SFT Model H6 (Avg.) ARC HellaSwag MMLU TruthfulQA Winogrande GSM8K

DPO v2 SFT v3 73.42 71.50 88.28 65.97 71.71 82.79 60.27 DPO v3 SFT v3 + v4 73.58 71.33 88.08 65.39 72.45 81.93 62.32

- Table 5: Ablation studies on the different SFT base models used during the direct preference optimization (DPO) stage. Ultrafeedback Clean and Synth. Math-Alignment datasets are used. We name ablated models with the ‘DPO’ prefix to indicate the alignment tuning stage. The best scores for H6 and the individual tasks are shown in bold.


indicate that using OpenOrca results in a model that behaves differently from using only Alpaca-GPT4.

Second, we investigate whether Synth. MathInstruct dataset is beneficial. For ‘SFT v3’, we add the Synth. Math-Instruct dataset, which boosts GSM8K scores to 64.14 and achieves comparable scores for the other tasks. Interestingly, when we add the Synth. Math-Instruct dataset to ‘SFT v1’ to train ‘SFT v4’, we get our highest H6 score of 70.88 with higher scores than ‘SFT v3’ for all tasks. From the above, we can see that adding the Synth. Math-Instruct dataset is helpful.

Lastly, we see whether merging models trained with and without OpenOrca can boost performance. In the first analysis, we saw that using OpenOrca resulted in a model that behaved differently from the model that was trained without OpenOrca. Building on this intuition, we merge ‘SFT v3’ and ‘SFT v4’ as they are the best-performing models with and without OpenOrca. To our surprise, the resulting merged model ‘SFT v3+v4’ retains the high scores for non-GSM8K tasks from ‘SFT v4’ but also achieves a higher GSM8K score than ‘SFT v3’ or ‘SFT v4’. Thus, we see that merging models that specialize in different tasks is a promising way to obtain a model that performs well generally.

# 4.3.2 Alignment Tuning

As we utilize DPO for practical alignment tuning, there are additional aspects to ablate such as the SFT base models used. Thus, we present ablations for the different training datasets used for training, the different SFT base models to initialize the DPO model, and finally, the model merging strategy to obtain the final alignment-tuned model.

Ablation on the training datasets. We ablate on the different alignment datasets used during DPO in Tab. 4. We use ‘SFT v3’ as the SFT base model for DPO. ‘DPO v1’ only uses the Ultrafeedback Clean dataset while ‘DPO v2’ also used the Synth. Math-Alignment dataset.

First, we test how Ultrafeedback Clean and Synth. Math-Alignment impacts model performance. For ‘DPO v1’, it achieves 73.06 in H6, which is a substantial boost from the SFT base model score of 70.03. However, we note that while scores for tasks like ARC, HellaSwag, and TruthfulQA all improved by good margins, the score for GSM8K is 58.83, which is lower than the SFT base model score of 64.14. Adding Synth. Math-Alignment to train ‘DPO v2’, we see that the GSM8k score improves to 60.27, which is lower than the SFT base model but still higher than ‘DPO v1’. Other task scores are also not nega-

