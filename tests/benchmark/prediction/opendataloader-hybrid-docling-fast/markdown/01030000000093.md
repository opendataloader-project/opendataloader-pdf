survey responses and outcomes from the experiments and games. This spreadsheet is linked to the students’ randomly assigned course ID (CID) numbers. The other spreadsheet, which is linked to their university student ID numbers and their names, compiles their performances on quizzes, homework, and exams assigned throughout the semester.

At the risk of sounding draconian, this is a course where it may make sense to base upwards of 50% of a student’s grade upon their in-person attendance, which would entail carefully taking role at the beginning of each class. If the class meets 30 times face-to-face during the semester, for example, their grade attributable to attendance would then drop by 3.33 percentage points for each missed class (excused absences withstanding). Granted, students who foresee having difficulty attending class in-person throughout the semester would likely choose to drop the course immediately. For those students who remain, the remaining 50% of their course grade would then be based upon their quizzes, homework, and exam scores.

The issue of how best to convey written information to the student a priori (i.e., before conducting a given experiment or game) also looms large in a participatory-learning setting such as this, especially if the instructor desires to obtain unbiased responses from the students (or more practically, to control for potential biases). For example, the first set of thought experiments presented in Section 1 is meant to demonstrate firsthand to the students the extent to which automatic, knee-jerk responses from what Kahneman (2011) identifies as the System 1 portion of the brain can result in miscalculations. Students who choose to read ahead (small in number though these types of students may be) potentially skew the distribution of responses away from its otherwise true representation of these miscalculations. Such skewness may be tolerable for strictly educational purposes, where the goal is to demonstrate that at least a certain percentage of students are prone to miscalculation. But if the instructor also hopes to compile student responses into a dataset amenable for statistical analysis, then this type of potential bias draws into question the validity of the data.2

To help control for potential biases associated with students having read ahead about the game or experiment they are now participating in, I recommend including the following question on each Response Card: “Did you read about this topic ahead of time?” (see Appendix A). Answers to this question provide a control for the level of student foreknowledge, which is the potential bias of concern.

I am personally unaware of any studies that have looked at how well students learn the lessons of behavioral economics in a cumulative sense over a span of time (e.g., an entire semester) and across a variety of experiments and games. In other words, I know of no studies that estimate the extent to which individuals who begin a course in behavioral economics as bona fide Homo sapiens evolve toward “Homo economism” in their individual and social choices. The pedagogy promoted in this textbook—in particular, the data it generates—offers instructors the opportunity to empirically test the hypothesis that students make this evolution.

2. Note that this potential biasedness problem also extends to the laboratory experiments of Section 2 and games of Section 3.

BEHAVIORAL ECONOMICS PRACTICUM XXV

